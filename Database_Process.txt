Getting Workable Database From MySQL Dump:

Problem 1:
File is a MySQL dump that is inaccessible due to scale (160gb file with data on 108 million users)
Convert MySQL into usable formats.
Insufficient Memory.

Solution:
Step 1) Create python script that will read the MySQL files in chunks. Find the last insert statement, which will be incomplete, then delete it. Save file as sample to create a working chunk size.
Step 2) Manually identify the schema of the tables and rewrite in working format for sqlite. Insert statements are same, therefore don't require editing.
Step 3) Run script that will convert all MySQL files into into Sqlite files using the defined chunk size to bypass insufficient memory issues.

Problem 2:
Since data contains user defined usernames, some people created names that would purposefully break code. This meant if a single person create a name ending in "\\" or "','" the insert statement would fail and we would lose about 18,000 records. We lost about 50% of the data due to these errors.

Solutiion:
Step 1) Create a "sanitizing" script that will be able to find the errors and replace them with a correction.
	- This process has it's own issues, but the script managed to save about 95% of the original data.
Step 2) Run a script that would convert the new Sqlite files into "sanitized" sqlite files.

Problem 3:
Getting large sqlite files to write to database requires too much memory.

Solutiion:
Step 1) Download sqlite3.exe and put file into working directory.
Step 2) From Powershell, connect to sqlite3 & database.
Step 3) Read the large sql files into database using command line.
	- This bypasses the memory requirements when working in other programs.

Problem 4:
Database size is too large for cleaning.

Solution:
When creating and dropping data to large tables, make sure to use powershell. Can automate process for small tables with vscode.
	- After database is cleaned, queries become possible in vscode to no longer needing to read large data entirely into memory.
	
Problem 5:
Database did not decrease after cleaning, pathing issues using multiple drives, and distribution to team.

Solution:
When data is dropped from an SQL database, the data is not fully deleted. The database instead stores values into deleted space to preserve the allocated space.
After cleaning database, you need to run the vacuum command to free up the preserved disk space.
This process requires space about equal to database size in order to run. Since my SSD had no space I moved to HDD, however my temporary folders were still on SSD.
In powershell I had to create a temporary temp directory. $env.TMP "directory", then within the same powershell window, run the vacuum.

Problem 6:
Games_1 and Friends use very large tables with slow query times.

Solution:
Create indicies for all the tables on AppID and SteamID for faster queries.

Problem 7:
There is not geojson for the steam data to plot on to a map.

Solution:
Find a geojson for all the countries in the world. https://github.com/johan/world.geo.json/blob/master/countries.geo.json
Get a csv file that contains ISO Alpha 2 and ISO Alpha 3 data. https://gist.github.com/tadast/8827699#file-countries_codes_and_coordinates-csv
Combine Steam's LocCountryCode's ISO Alpha 2, to the CSV's Alpha 2.
Delete the ISO Alpha 2 column, Numeric, Lat, Long columns so Steam data now has Alpha 3 codes to pair with the geojson